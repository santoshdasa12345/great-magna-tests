version: 2
jobs:
  domestic_compare_stage_and_uat_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: domestic
    - ENVS_TO_COMPARE: stage_uat
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  load_fab_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: fab
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  smoke_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: stage
    - PYTEST_ARGS: -m 'not dev and not uat and not prod'
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - smoke-tests-dependency-cache-{{ .Revision }}
    - run:
        name: Run smoke tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export
          PYTEST_ARGS=${PYTEST_ARGS} make smoke_tests
    - run:
        name: Install moxci to send CircleCI notifacations to Slack
        when: always
        command: |
          sudo npm install -g moxci@0.1.3
    - run:
        name: Install Allure command line tool
        when: always
        command: |
          wget https://github.com/allure-framework/allure2/releases/download/2.13.1/allure-commandline-2.13.1.zip -O allure.zip
          unzip -q allure.zip
          find . -maxdepth 1 -type d -name "allure*" -exec mv {} allure \;
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Generate Allure report
        when: always
        command: |
          export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
          export PATH=$PATH:$HOME/bin:$PATH:$JAVA_HOME/bin:./allure/bin
          make report
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
    - store_artifacts:
        path: ./allure_report
        destination: allure_report
    - run:
        name: Push Allure report link to Slack with moxci
        when: always
        command: |
          set -e
          export CIRCLE_PULL_REQUEST="${CIRCLE_PULL_REQUEST:-none}"
          export WORKFLOW_LINK="https://circleci.com/workflow-run/${CIRCLE_WORKFLOW_ID}"
          export summary="$(./parse_test_summary_json.py allure_report/widgets/summary.json)"
          moxci allure_report/index.html --slack_message "The latest report from SMOKE tests ran against `awk '/Environment/{print toupper($3)}' ./results/environment.properties`. ${summary}. (workflow â†’ ${WORKFLOW_LINK})"
  load_profile_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: profile
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  setup_env_for_smoke_tests:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - checkout
    - run:
        name: Create virtualenv and install dependencies
        command: |
          python3 -m venv venv
          . venv/bin/activate
          pip install --quiet --upgrade pip
          pip install --quiet -r requirements_smoke.txt
    - save_cache:
        key: smoke-tests-dependency-cache-{{ .Revision }}
        paths:
        - ~/great_magna_tests
  load_invest_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: invest
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  check_for_dead_links_on_prod:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: PROD
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Dead Links Checker script
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env_dead_link_checker.json
          source ./env_vars/.env_with_export;
          TEST_ENV=${TEST_ENV} make dead_links_check
          make dead_links_list > ./reports/dead_links_report.txt
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  load_domestic_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: domestic
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  func_profile_test_stage:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: stage
    - FEATURE_DIR: profile
    - TAGS: --tags=~@dev-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_profile
  smoke_tests_dev:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: dev
    - PYTEST_ARGS: -m "not stage and not uat and not prod"
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - smoke-tests-dependency-cache-{{ .Revision }}
    - run:
        name: Run smoke tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export
          PYTEST_ARGS=${PYTEST_ARGS} make smoke_tests
    - run:
        name: Install moxci to send CircleCI notifacations to Slack
        when: always
        command: |
          sudo npm install -g moxci@0.1.3
    - run:
        name: Install Allure command line tool
        when: always
        command: |
          wget https://github.com/allure-framework/allure2/releases/download/2.13.1/allure-commandline-2.13.1.zip -O allure.zip
          unzip -q allure.zip
          find . -maxdepth 1 -type d -name "allure*" -exec mv {} allure \;
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Generate Allure report
        when: always
        command: |
          export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
          export PATH=$PATH:$HOME/bin:$PATH:$JAVA_HOME/bin:./allure/bin
          make report
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
    - store_artifacts:
        path: ./allure_report
        destination: allure_report
    - run:
        name: Push Allure report link to Slack with moxci
        when: always
        command: |
          set -e
          export CIRCLE_PULL_REQUEST="${CIRCLE_PULL_REQUEST:-none}"
          export WORKFLOW_LINK="https://circleci.com/workflow-run/${CIRCLE_WORKFLOW_ID}"
          export summary="$(./parse_test_summary_json.py allure_report/widgets/summary.json)"
          moxci allure_report/index.html --slack_message "The latest report from SMOKE tests ran against `awk '/Environment/{print toupper($3)}' ./results/environment.properties`. ${summary}. (workflow â†’ ${WORKFLOW_LINK})"
  generate_allure_report_from_functional_tests_and_send_link_to_slack:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - checkout
    - attach_workspace:
        at: ./
    - run:
        name: Install moxci to send CircleCI notifacations to Slack
        when: always
        command: |
          sudo npm install -g moxci@0.1.3
    - run:
        name: Install Allure command line tool
        when: always
        command: |
          wget https://github.com/allure-framework/allure2/releases/download/2.13.1/allure-commandline-2.13.1.zip -O allure.zip
          unzip -q allure.zip
          find . -maxdepth 1 -type d -name "allure*" -exec mv {} allure \;
    - run:
        name: Update Allure result files generated by Functional tests
        when: always
        command: |
          set -e
          ls -lad results*
          make results_functional
    - run:
        name: Generate Allure report
        when: always
        command: |
          export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
          export PATH=$PATH:$HOME/bin:$PATH:$JAVA_HOME/bin:./allure/bin
          make report
    - store_artifacts:
        path: ./allure_report/
        destination: allure_report
    - run:
        name: Push Allure report link to Slack with moxci
        when: always
        command: |
          set -e
          export CIRCLE_PULL_REQUEST="${CIRCLE_PULL_REQUEST:-none}"
          export WORKFLOW_LINK="https://circleci.com/workflow-run/${CIRCLE_WORKFLOW_ID}"
          export summary="$(./parse_test_summary_json.py allure_report/widgets/summary.json)"
          moxci allure_report/index.html --slack_message "The latest report from FUNCTIONAL tests ran against `awk '/Environment/{print toupper($3)}' ./results/environment.properties`. ${summary}. (workflow â†’ ${WORKFLOW_LINK})"
  delete_test_data_uat:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: uat
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Delete form submissions, SSO accounts & companies created by automated tests
        when: always
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          ./delete_test_data.py
  check_for_dead_links_on_uat:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: UAT
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Dead Links Checker script
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env_dead_link_checker.json
          source ./env_vars/.env_with_export;
          TEST_ENV=${TEST_ENV} make dead_links_check
          make dead_links_list > ./reports/dead_links_report.txt
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  delete_test_data_stage:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: stage
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Delete form submissions, SSO accounts & companies created by automated tests
        when: always
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          ./delete_test_data.py
  check_for_dead_links_on_dev:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: DEV
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Dead Links Checker script
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env_dead_link_checker.json
          source ./env_vars/.env_with_export;
          TEST_ENV=${TEST_ENV} make dead_links_check
          make dead_links_list > ./reports/dead_links_report.txt
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  func_international_test_stage:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: stage
    - FEATURE_DIR: international
    - TAGS: --tags=~@dev-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_international
  func_international_test_dev:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: dev
    - FEATURE_DIR: international
    - TAGS: --tags=~@stage-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_international
  refresh_geckoboard:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Geckoboard Updater script
        command: |
          . venv/bin/activate
          make geckoboard_updater
  check_for_dead_links_on_stage:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: STAGE
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Dead Links Checker script
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env_dead_link_checker.json
          source ./env_vars/.env_with_export;
          TEST_ENV=${TEST_ENV} make dead_links_check
          make dead_links_list > ./reports/dead_links_report.txt
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  func_profile_test_dev:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: dev
    - FEATURE_DIR: profile
    - TAGS: --tags=~@stage-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_profile
  international_compare_prod_and_uat_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: international
    - ENVS_TO_COMPARE: prod_uat
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  func_sso_test_stage:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: stage
    - FEATURE_DIR: sso
    - TAGS: --tags=~@dev-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_sso
  func_fas_test_uat:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: uat
    - FEATURE_DIR: fas
    - TAGS: --tags=~@stage-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_fas
  international_compare_stage_and_uat_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: international
    - ENVS_TO_COMPARE: stage_uat
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  load_soo_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: soo
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  merge_junit_results_from_parallel_browser_tests:
    docker:
    - image: circleci/python:3.8.0-node-browsers
    working_directory: ~/great_magna_tests
    steps:
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - checkout
    - attach_workspace:
        at: ./
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          pip install --quiet --upgrade pip
          cd ./tests/browser/
          mkdir junit_report
          mv junit_reports_* junit_report/
          ls -la junit_report/
          junit-merge --recursive --dir junit_report/ --out ./junit_report/merged.xml
          rm -fr junit_report/junit_reports_*
          cd ../../
          ./print_error_summary.py --report ./tests/browser/junit_report/merged.xml
    - store_test_results:
        path: ./tests/browser/junit_report/
    - store_artifacts:
        path: ./tests/browser/junit_report/
        destination: junit_report
  delete_test_data_dev:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: dev
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Delete form submissions, SSO accounts & companies created by automated tests
        when: always
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          ./delete_test_data.py
  setup_env_for_load_tests:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - checkout
    - run:
        name: Create virtualenv and install dependencies
        command: |
          python3 -m venv venv
          . venv/bin/activate
          pip install --quiet --upgrade pip
          pip install --quiet -r requirements_load.txt
    - save_cache:
        key: load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
        paths:
        - ~/great_magna_tests
  load_erp_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: erp
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  func_international_test_uat:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: uat
    - FEATURE_DIR: international
    - TAGS: --tags=~@stage-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_international
  uat_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    parallelism: 12
    environment:
    - TEST_ENV: uat
    - AUTO_RETRY: 'true'
    - TAKE_SCREENSHOTS: 'true'
    - TAGS: --tags=~@stage-only --tags=~@erp
    - BROWSER_ENVIRONMENT: local
    - BROWSER_HEADLESS: 'true'
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - browser-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Prepare a list of scenarios to run
        command: |
          pwd
          . venv/bin/activate
          source ./env_vars/env_with_export.txt
          export PATH=$PATH:$HOME/bin
          if [ $(( ${CIRCLE_NODE_INDEX} % 2 )) = 0 ];
          then
              export BROWSER="chrome";
          else
              export BROWSER="firefox";
          fi
          cd tests/browser
    - run:
        name: Run browser tests in parallel
        command: |
          . venv/bin/activate
          source ./env_vars/env_with_export.txt
          export PATH=$PATH:$HOME/bin
          if [ $(( ${CIRCLE_NODE_INDEX} % 2 )) = 0 ];
          then
              export BROWSER="chrome";
              google-chrome --version;
          else
              export BROWSER="firefox";
              firefox --version;
              export TAGS="${TAGS} --tags=~@skip-in-firefox"
          fi

          echo "Creating results output directory"
          mkdir results_${BROWSER}_${CIRCLE_NODE_INDEX}

          cd tests/browser
          echo "Found `wc -l < scenario_titles.txt` scenarios to run"
          # cat scenario_titles.txt | circleci tests split > /tmp/tests-to-run
          # calculate number of lines after which list of scenarios will be split
          #      SPLIT_AFTER_LINES=$(( $(wc -l < scenario_titles.txt) / $((${CIRCLE_NODE_TOTAL} / 2)) ))
          SPLIT_AFTER_LINES=$(( $(wc -l < scenario_titles.txt)))
          #       split list of scenarios into equal parts (equal to half on number of nodes)
          # as half of nodes run tests in Chrome & the other half in Firefox
          split -d --suffix-length=1 --lines ${SPLIT_AFTER_LINES} scenario_titles.txt tests-to-run

          #SPLIT_FILE_SUFFIX=$(( ${CIRCLE_NODE_INDEX} / 2 ))
          SPLIT_FILE_SUFFIX=$(( ${CIRCLE_NODE_INDEX} / 2 ))

          echo "Creating results output directory inside tests/browser"
          mkdir results_${BROWSER}_${CIRCLE_NODE_INDEX}/
          echo "Current instance of ${BROWSER} is going to run `cat tests-to-run${SPLIT_FILE_SUFFIX} | wc -l` scenarios:"
          cat tests-to-run${SPLIT_FILE_SUFFIX}

          cat tests-to-run${SPLIT_FILE_SUFFIX} | while IFS=$'\r' read -r title ; do
            echo -e "\n\n${BROWSER} run:    ${title}"
            PYTHONPATH=. \
            LOG_FILE="reports/behave.log" \
            timeout --verbose --preserve-status --kill-after=5s 5m \
            behave features/ \
            --format=allure_behave.formatter:AllureFormatter \
            --define AllureFormatter.issue_pattern=${BUG_TRACKER_URL_PATTERN} \
            --define AllureFormatter.link_pattern=${BUG_TRACKER_URL_PATTERN} \
            --outfile=results_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
            --no-skipped \
            --no-source \
            --no-summary \
            --junit \
            --junit-directory=./junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
            --tags=~@wip \
            --tags=~@fixme \
            --tags=~@skip \
            --tags=~@eu-exit \
            ${TAGS} \
            --name "${title}" && {
              echo "${BROWSER} passed: ${title}"
            } || {
              echo "${BROWSER} failed: ${title}"
            }
          done
          echo "Found `ls -la results_${BROWSER}_${CIRCLE_NODE_INDEX}/*.json | wc -l` JSON Allure result files in results_${BROWSER}_${CIRCLE_NODE_INDEX}/"
          echo "Found `ls -la junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/*.xml | wc -l` JUnit XML result files in junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/"

          echo "Saving environment variables that control test execution"
          ../../save_test_related_env_vars.py
    - persist_to_workspace:
        root: .
        paths:
        - .
  load_cms_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: cms
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  load_international_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: international
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  domestic_compare_stage_and_dev_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: domestic
    - ENVS_TO_COMPARE: stage_dev
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  setup_env_for_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - checkout
    - run:
        name: Create virtualenv and install dependencies
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          pip install --quiet --upgrade pip
          pip install --quiet -r requirements_browser.txt
    - save_cache:
        key: browser-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
        paths:
        - ~/great_magna_tests
  generate_cms_page_status_report_for_prod:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: PROD
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Generate CMS Page Status report
        command: |
          . venv/bin/activate
          make cms_page_status_report
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  load_fas_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: fas
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  domestic_compare_prod_and_dev_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: domestic
    - ENVS_TO_COMPARE: prod_dev
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  international_compare_stage_and_dev_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: international
    - ENVS_TO_COMPARE: stage_dev
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  load_isd_tests_stage:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: isd
    - TEST_ENV: stage
    - HATCH_RATE: 3
    - NUM_USERS: 1000
    - RUN_TIME: 5m
    docker:
    - image: circleci/python:3.8.0-node
    steps:
    - restore_cache:
        keys:
        - load-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run load tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          NUM_USERS=${NUM_USERS} HATCH_RATE=${HATCH_RATE} RUN_TIME=${RUN_TIME} make load_test_${SERVICE}
    - store_artifacts:
        path: ./reports
  func_fas_test_dev:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: dev
    - FEATURE_DIR: fas
    - TAGS: --tags=~@stage-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_fas
  domestic_compare_prod_and_uat_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: domestic
    - ENVS_TO_COMPARE: prod_uat
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  domestic_compare_prod_and_stage_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: domestic
    - ENVS_TO_COMPARE: prod_stage
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  func_sso_test_uat:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: uat
    - FEATURE_DIR: sso
    - TAGS: --tags=~@stage-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_sso
  merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - checkout
    - attach_workspace:
        at: ./
    - run:
        name: Install moxci to send CircleCI notifacations to Slack
        when: always
        command: |
          sudo npm install -g moxci@0.1.3
    - run:
        name: Install Allure command line tool
        when: always
        command: |
          wget https://github.com/allure-framework/allure2/releases/download/2.13.1/allure-commandline-2.13.1.zip -O allure.zip
          unzip -q allure.zip
          find . -maxdepth 1 -type d -name "allure*" -exec mv {} allure \;
    - run:
        name: Update Allure result files generated by Browser tests
        when: always
        command: |
          set -e
          mv tests/browser/results* .
          ls -lad results*
          make results_browser
    - run:
        name: Generate Allure report
        when: always
        command: |
          export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
          export PATH=$PATH:$HOME/bin:$PATH:$JAVA_HOME/bin:./allure/bin
          make report
    - store_artifacts:
        path: ./allure_report/
        destination: allure_report
    - run:
        name: Push Allure report link to Slack with moxci
        when: always
        command: |
          set -e
          export CIRCLE_PULL_REQUEST="${CIRCLE_PULL_REQUEST:-none}"
          export WORKFLOW_LINK="https://circleci.com/workflow-run/${CIRCLE_WORKFLOW_ID}"
          export summary="$(./parse_test_summary_json.py allure_report/widgets/summary.json)"
          moxci allure_report/index.html --slack_message "The latest report from BROWSER tests ran against `awk '/Environment/{print toupper($3)}' ./results/environment.properties`. ${summary}. (workflow â†’ ${WORKFLOW_LINK})"
  setup_env_for_periodic_tasks:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0
    steps:
    - checkout
    - run:
        name: Create virtualenv and install dependencies
        command: |
          python3 -m venv venv
          . venv/bin/activate
          pip install --upgrade pip
          pip install -q -r requirements_periodic_tasks.txt
    - save_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
        paths:
        - ~/great_magna_tests
  smoke_tests_uat:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: uat
    - PYTEST_ARGS: -m 'not dev and not stage and not prod'
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - smoke-tests-dependency-cache-{{ .Revision }}
    - run:
        name: Run smoke tests
        command: |
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export
          PYTEST_ARGS=${PYTEST_ARGS} make smoke_tests
    - run:
        name: Install moxci to send CircleCI notifacations to Slack
        when: always
        command: |
          sudo npm install -g moxci@0.1.3
    - run:
        name: Install Allure command line tool
        when: always
        command: |
          wget https://github.com/allure-framework/allure2/releases/download/2.13.1/allure-commandline-2.13.1.zip -O allure.zip
          unzip -q allure.zip
          find . -maxdepth 1 -type d -name "allure*" -exec mv {} allure \;
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Generate Allure report
        when: always
        command: |
          export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
          export PATH=$PATH:$HOME/bin:$PATH:$JAVA_HOME/bin:./allure/bin
          make report
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
    - store_artifacts:
        path: ./allure_report
        destination: allure_report
    - run:
        name: Push Allure report link to Slack with moxci
        when: always
        command: |
          set -e
          export CIRCLE_PULL_REQUEST="${CIRCLE_PULL_REQUEST:-none}"
          export WORKFLOW_LINK="https://circleci.com/workflow-run/${CIRCLE_WORKFLOW_ID}"
          export summary="$(./parse_test_summary_json.py allure_report/widgets/summary.json)"
          moxci allure_report/index.html --slack_message "The latest report from SMOKE tests ran against `awk '/Environment/{print toupper($3)}' ./results/environment.properties`. ${summary}. (workflow â†’ ${WORKFLOW_LINK})"
  setup_env_for_functional_tests:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - checkout
    - run:
        name: Create virtualenv and install dependencies
        command: |
          python3 -m venv venv
          . venv/bin/activate
          pip install --quiet --upgrade pip
          pip install --quiet -r requirements_functional.txt
          sudo npm install -g junit-merge@2.0.0
    - save_cache:
        key: functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
        paths:
        - ~/great_magna_tests
  dev_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    parallelism: 12
    environment:
    - TEST_ENV: dev
    - AUTO_RETRY: 'true'
    - TAKE_SCREENSHOTS: 'true'
    - TAGS: --tags=~@stage-only --tags=~@uat-only --tags=~@erp
    - BROWSER_ENVIRONMENT: local
    - BROWSER_HEADLESS: 'true'
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - checkout
    - run:
        name: Create virtualenv and install dependencies
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          pip install --quiet --upgrade pip
          pip install --quiet -r requirements_browser.txt
    - run:
        name: Change to great_magna_tests directory
        when: always
        command: |
          cd /home/circleci/great_magna_tests
    - run:
        name: Prepare a list of scenarios to run
        command: |
          pwd
          . venv/bin/activate
          source ./env_vars/env_with_export.txt
          export PATH=$PATH:$HOME/bin
          if [ $(( ${CIRCLE_NODE_INDEX} % 2 )) = 0 ];
          then
              export BROWSER="chrome";
          else
              export BROWSER="firefox";
          fi
          cd tests/browser
    - run:
        name: Run browser tests in parallel
        command: |
          . venv/bin/activate
          source ./env_vars/env_with_export.txt
          export PATH=$PATH:$HOME/bin
          if [ $(( ${CIRCLE_NODE_INDEX} % 2 )) = 0 ];
          then
              export BROWSER="chrome";
              google-chrome --version;
          else
              export BROWSER="firefox";
              firefox --version;
              export TAGS="${TAGS} --tags=~@skip-in-firefox"
          fi

          echo "Creating results output directory"
          mkdir results_${BROWSER}_${CIRCLE_NODE_INDEX}

          cd tests/browser
          echo "Found `wc -l < scenario_titles.txt` scenarios to run"
          # cat scenario_titles.txt | circleci tests split > /tmp/tests-to-run
          # calculate number of lines after which list of scenarios will be split
          #      SPLIT_AFTER_LINES=$(( $(wc -l < scenario_titles.txt) / $((${CIRCLE_NODE_TOTAL} / 2)) ))
          SPLIT_AFTER_LINES=$(( $(wc -l < scenario_titles.txt)))
          #       split list of scenarios into equal parts (equal to half on number of nodes)
          # as half of nodes run tests in Chrome & the other half in Firefox
          split -d --suffix-length=1 --lines ${SPLIT_AFTER_LINES} scenario_titles.txt tests-to-run

          #SPLIT_FILE_SUFFIX=$(( ${CIRCLE_NODE_INDEX} / 2 ))
          SPLIT_FILE_SUFFIX=$(( ${CIRCLE_NODE_INDEX} / 2 ))

          echo "Creating results output directory inside tests/browser"
          mkdir results_${BROWSER}_${CIRCLE_NODE_INDEX}/
          echo "Current instance of ${BROWSER} is going to run `cat tests-to-run${SPLIT_FILE_SUFFIX} | wc -l` scenarios:"
          cat tests-to-run${SPLIT_FILE_SUFFIX}

          cat tests-to-run${SPLIT_FILE_SUFFIX} | while IFS=$'\r' read -r title ; do
            echo -e "\n\n${BROWSER} run:    ${title}"
            PYTHONPATH=. \
            LOG_FILE="reports/behave.log" \
            timeout --verbose --preserve-status --kill-after=5s 5m \
            behave features/ \
            --format=allure_behave.formatter:AllureFormatter \
            --define AllureFormatter.issue_pattern=${BUG_TRACKER_URL_PATTERN} \
            --define AllureFormatter.link_pattern=${BUG_TRACKER_URL_PATTERN} \
            --outfile=results_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
            --no-skipped \
            --no-source \
            --no-summary \
            --junit \
            --junit-directory=./junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
            --tags=~@wip \
            --tags=~@fixme \
            --tags=~@skip \
            --tags=~@eu-exit \
            ${TAGS} \
            --name "${title}" && {
              echo "${BROWSER} passed: ${title}"
            } || {
              echo "${BROWSER} failed: ${title}"
            }
          done
          echo "Found `ls -la results_${BROWSER}_${CIRCLE_NODE_INDEX}/*.json | wc -l` JSON Allure result files in results_${BROWSER}_${CIRCLE_NODE_INDEX}/"
          echo "Found `ls -la junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/*.xml | wc -l` JUnit XML result files in junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/"

          echo "Saving environment variables that control test execution"
          ../../save_test_related_env_vars.py
    - persist_to_workspace:
        root: .
        paths:
        - .
  func_sso_test_dev:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: dev
    - FEATURE_DIR: sso
    - TAGS: --tags=~@stage-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_sso
  func_fas_test_stage:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: stage
    - FEATURE_DIR: fas
    - TAGS: --tags=~@dev-only --tags=~@uat-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_fas
  check_cms_pages_on_production_return_200:
    working_directory: ~/great_magna_tests
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run CMS Checker
        command: |
          . venv/bin/activate
          make test_cms_pages_return_200
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  international_compare_prod_and_stage_pages:
    working_directory: ~/great_magna_tests
    environment:
    - SERVICE: international
    - ENVS_TO_COMPARE: prod_stage
    docker:
    - image: circleci/python:3.8.0
    steps:
    - restore_cache:
        key: periodic-tasks-dependency-cache-{{ .Revision }}
    - run:
        name: Run Content Diff tests
        command: |
          . venv/bin/activate
          SERVICE=${SERVICE} ENVS_TO_COMPARE=${ENVS_TO_COMPARE} make compare_content
    - store_test_results:
        path: ./reports
    - store_artifacts:
        path: ./reports
  stage_parallel_browser_tests_using_circleci:
    working_directory: ~/great_magna_tests
    parallelism: 12
    environment:
    - TEST_ENV: stage
    - AUTO_RETRY: 'true'
    - TAKE_SCREENSHOTS: 'true'
    - TAGS: --tags=~@dev-only --tags=~@uat-only --tags=~@erp
    - BROWSER_ENVIRONMENT: local
    - BROWSER_HEADLESS: 'true'
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - browser-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Prepare a list of scenarios to run
        command: |
          pwd
          . venv/bin/activate
          source ./env_vars/env_with_export.txt
          export PATH=$PATH:$HOME/bin
          if [ $(( ${CIRCLE_NODE_INDEX} % 2 )) = 0 ];
          then
              export BROWSER="chrome";
          else
              export BROWSER="firefox";
          fi
          cd tests/browser
    - run:
        name: Run browser tests in parallel
        command: |
          . venv/bin/activate
          source ./env_vars/env_with_export.txt
          export PATH=$PATH:$HOME/bin
          if [ $(( ${CIRCLE_NODE_INDEX} % 2 )) = 0 ];
          then
              export BROWSER="chrome";
              google-chrome --version;
          else
              export BROWSER="firefox";
              firefox --version;
              export TAGS="${TAGS} --tags=~@skip-in-firefox"
          fi

          echo "Creating results output directory"
          mkdir results_${BROWSER}_${CIRCLE_NODE_INDEX}

          cd tests/browser
          echo "Found `wc -l < scenario_titles.txt` scenarios to run"
          # cat scenario_titles.txt | circleci tests split > /tmp/tests-to-run
          # calculate number of lines after which list of scenarios will be split
          #      SPLIT_AFTER_LINES=$(( $(wc -l < scenario_titles.txt) / $((${CIRCLE_NODE_TOTAL} / 2)) ))
          SPLIT_AFTER_LINES=$(( $(wc -l < scenario_titles.txt)))
          #       split list of scenarios into equal parts (equal to half on number of nodes)
          # as half of nodes run tests in Chrome & the other half in Firefox
          split -d --suffix-length=1 --lines ${SPLIT_AFTER_LINES} scenario_titles.txt tests-to-run

          #SPLIT_FILE_SUFFIX=$(( ${CIRCLE_NODE_INDEX} / 2 ))
          SPLIT_FILE_SUFFIX=$(( ${CIRCLE_NODE_INDEX} / 2 ))

          echo "Creating results output directory inside tests/browser"
          mkdir results_${BROWSER}_${CIRCLE_NODE_INDEX}/
          echo "Current instance of ${BROWSER} is going to run `cat tests-to-run${SPLIT_FILE_SUFFIX} | wc -l` scenarios:"
          cat tests-to-run${SPLIT_FILE_SUFFIX}

          cat tests-to-run${SPLIT_FILE_SUFFIX} | while IFS=$'\r' read -r title ; do
            echo -e "\n\n${BROWSER} run:    ${title}"
            PYTHONPATH=. \
            LOG_FILE="reports/behave.log" \
            timeout --verbose --preserve-status --kill-after=5s 5m \
            behave features/ \
            --format=allure_behave.formatter:AllureFormatter \
            --define AllureFormatter.issue_pattern=${BUG_TRACKER_URL_PATTERN} \
            --define AllureFormatter.link_pattern=${BUG_TRACKER_URL_PATTERN} \
            --outfile=results_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
            --no-skipped \
            --no-source \
            --no-summary \
            --junit \
            --junit-directory=./junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/ \
            --tags=~@wip \
            --tags=~@fixme \
            --tags=~@skip \
            --tags=~@eu-exit \
            ${TAGS} \
            --name "${title}" && {
              echo "${BROWSER} passed: ${title}"
            } || {
              echo "${BROWSER} failed: ${title}"
            }
          done
          echo "Found `ls -la results_${BROWSER}_${CIRCLE_NODE_INDEX}/*.json | wc -l` JSON Allure result files in results_${BROWSER}_${CIRCLE_NODE_INDEX}/"
          echo "Found `ls -la junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/*.xml | wc -l` JUnit XML result files in junit_reports_${BROWSER}_${CIRCLE_NODE_INDEX}/"

          echo "Saving environment variables that control test execution"
          ../../save_test_related_env_vars.py
    - persist_to_workspace:
        root: .
        paths:
        - .
  func_profile_test_uat:
    working_directory: ~/great_magna_tests
    environment:
    - TEST_ENV: uat
    - FEATURE_DIR: profile
    - TAGS: --tags=~@stage-only
    docker:
    - image: circleci/python:3.8.0-node-browsers
    steps:
    - restore_cache:
        keys:
        - functional-tests-dependency-cache-{{ .Environment.CIRCLE_WORKFLOW_ID }}
    - run:
        name: Run functional tests
        command: |
          python3 -m venv venv
          . venv/bin/activate
          python ./env_vars/env_writer.py --env=${TEST_ENV} --config=./env_vars/env.json
          source ./env_vars/.env_with_export;
          export PATH=$PATH:$HOME/bin
          FEATURE_DIR=${FEATURE_DIR} TAGS="${TAGS}" make functional_tests_feature_dir
    - run:
        name: Saving environment variables that control test execution
        when: always
        command: |
          . venv/bin/activate
          echo "Saving environment variables that control test execution"
          ./save_test_related_env_vars.py
    - run:
        name: Install an npm package to merge JUnit XML report files
        when: always
        command: |
          sudo npm install -g junit-merge@2.0.0
    - run:
        name: Merge JUnit XML reports & print errors summary
        when: always
        command: |
          python3 -m venv venv
          . venv/bin/activate
          cd ./tests/functional/reports
          junit-merge *.xml -o merged.xml
          ls *.xml | grep -v merged.xml | xargs rm
          cd ../../../
          ./print_error_summary.py --report ./tests/functional/reports/merged.xml
    - store_test_results:
        path: ./tests/functional/reports
    - store_artifacts:
        path: ./tests/functional/reports
    - persist_to_workspace:
        root: .
        paths:
        - results_profile
  store_behave_logs_from_browser_tests_as_artifacts:
    docker:
    - image: circleci/python:3.8.0
    working_directory: ~/great_magna_tests
    steps:
    - attach_workspace:
        at: ./
    - store_artifacts:
        path: ./tests/browser/reports/
        destination: behave_logs
workflows:
  version: 2
  run_smoke_and_functional_tests_on_dev_ad_hoc:
    jobs:
    - setup_env_for_smoke_tests
    - setup_env_for_functional_tests
    - delete_test_data_dev:
        requires:
        - setup_env_for_functional_tests
    - smoke_tests_dev:
        requires:
        - setup_env_for_smoke_tests
    - func_fas_test_dev:
        requires:
        - delete_test_data_dev
    - func_international_test_dev:
        requires:
        - delete_test_data_dev
    - func_sso_test_dev:
        requires:
        - delete_test_data_dev
    - func_profile_test_dev:
        requires:
        - delete_test_data_dev
    - generate_allure_report_from_functional_tests_and_send_link_to_slack:
        requires:
        - func_fas_test_dev
        - func_sso_test_dev
        - func_profile_test_dev
        - func_international_test_dev
  run_browser_tests_in_parallel_in_dev_ad_hoc:
    jobs:
    - setup_env_for_parallel_browser_tests_using_circleci
    - dev_parallel_browser_tests_using_circleci:
        requires:
        - setup_env_for_parallel_browser_tests_using_circleci
    - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
        requires:
        - dev_parallel_browser_tests_using_circleci
    - merge_junit_results_from_parallel_browser_tests:
        requires:
        - dev_parallel_browser_tests_using_circleci
    - store_behave_logs_from_browser_tests_as_artifacts:
        requires:
        - dev_parallel_browser_tests_using_circleci
  run_smoke_and_functional_on_stage_ad_hoc:
    jobs:
    - setup_env_for_smoke_tests
    - setup_env_for_functional_tests
    - delete_test_data_stage:
        requires:
        - setup_env_for_functional_tests
    - smoke_tests_stage:
        requires:
        - setup_env_for_smoke_tests
    - func_fas_test_stage:
        requires:
        - delete_test_data_stage
    - func_international_test_stage:
        requires:
        - delete_test_data_stage
    - func_sso_test_stage:
        requires:
        - delete_test_data_stage
    - func_profile_test_stage:
        requires:
        - delete_test_data_stage
    - generate_allure_report_from_functional_tests_and_send_link_to_slack:
        requires:
        - func_fas_test_stage
        - func_sso_test_stage
        - func_profile_test_stage
        - func_international_test_stage
  run_browser_tests_in_parallel_in_stage_ad_hoc:
    jobs:
    - setup_env_for_parallel_browser_tests_using_circleci
    - stage_parallel_browser_tests_using_circleci:
        requires:
        - setup_env_for_parallel_browser_tests_using_circleci
    - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
        requires:
        - stage_parallel_browser_tests_using_circleci
    - merge_junit_results_from_parallel_browser_tests:
        requires:
        - stage_parallel_browser_tests_using_circleci
    - store_behave_logs_from_browser_tests_as_artifacts:
        requires:
        - stage_parallel_browser_tests_using_circleci
  run_smoke_and_functional_on_uat_ad_hoc:
    jobs:
    - setup_env_for_smoke_tests
    - setup_env_for_functional_tests
    - delete_test_data_uat:
        requires:
        - setup_env_for_functional_tests
    - smoke_tests_uat:
        requires:
        - setup_env_for_smoke_tests
    - func_fas_test_uat:
        requires:
        - delete_test_data_uat
    - func_international_test_uat:
        requires:
        - delete_test_data_uat
    - func_sso_test_uat:
        requires:
        - delete_test_data_uat
    - func_profile_test_uat:
        requires:
        - delete_test_data_uat
    - generate_allure_report_from_functional_tests_and_send_link_to_slack:
        requires:
        - func_fas_test_uat
        - func_sso_test_uat
        - func_profile_test_uat
        - func_international_test_uat
  run_browser_tests_in_parallel_in_uat_ad_hoc:
    jobs:
    - setup_env_for_parallel_browser_tests_using_circleci
    - uat_parallel_browser_tests_using_circleci:
        requires:
        - setup_env_for_parallel_browser_tests_using_circleci
    - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
        requires:
        - uat_parallel_browser_tests_using_circleci
    - merge_junit_results_from_parallel_browser_tests:
        requires:
        - uat_parallel_browser_tests_using_circleci
    - store_behave_logs_from_browser_tests_as_artifacts:
        requires:
        - uat_parallel_browser_tests_using_circleci
  prod_check_for_dead_links_ad_hoc:
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_prod:
        requires:
        - setup_env_for_periodic_tasks
  uat_check_for_dead_links_ad_hoc:
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_uat:
        requires:
        - setup_env_for_periodic_tasks
  stage_check_for_dead_links_ad_hoc:
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_stage:
        requires:
        - setup_env_for_periodic_tasks
  dev_check_for_dead_links_ad_hoc:
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_dev:
        requires:
        - setup_env_for_periodic_tasks
  refresh_geckoboard_ad_hoc:
    jobs:
    - setup_env_for_periodic_tasks
    - refresh_geckoboard:
        requires:
        - setup_env_for_periodic_tasks
  run_smoke_and_functional_tests_on_dev:
    triggers:
    - schedule:
        cron: 7 1 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_smoke_tests
    - setup_env_for_functional_tests
    - delete_test_data_dev:
        requires:
        - setup_env_for_functional_tests
    - smoke_tests_dev:
        requires:
        - setup_env_for_smoke_tests
    - func_fas_test_dev:
        requires:
        - delete_test_data_dev
    - func_international_test_dev:
        requires:
        - delete_test_data_dev
    - func_sso_test_dev:
        requires:
        - delete_test_data_dev
    - func_profile_test_dev:
        requires:
        - delete_test_data_dev
    - generate_allure_report_from_functional_tests_and_send_link_to_slack:
        requires:
        - func_fas_test_dev
        - func_sso_test_dev
        - func_profile_test_dev
        - func_international_test_dev
  run_browser_tests_in_parallel_in_dev:
    triggers:
    - schedule:
        cron: 39 1 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_parallel_browser_tests_using_circleci
    - dev_parallel_browser_tests_using_circleci:
        requires:
        - setup_env_for_parallel_browser_tests_using_circleci
    - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
        requires:
        - dev_parallel_browser_tests_using_circleci
    - merge_junit_results_from_parallel_browser_tests:
        requires:
        - dev_parallel_browser_tests_using_circleci
    - store_behave_logs_from_browser_tests_as_artifacts:
        requires:
        - dev_parallel_browser_tests_using_circleci
  run_smoke_and_functional_tests_on_stage:
    triggers:
    - schedule:
        cron: 52 2 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_smoke_tests
    - setup_env_for_functional_tests
    - delete_test_data_stage:
        requires:
        - setup_env_for_functional_tests
    - smoke_tests_stage:
        requires:
        - setup_env_for_smoke_tests
    - func_fas_test_stage:
        requires:
        - delete_test_data_stage
    - func_international_test_stage:
        requires:
        - delete_test_data_stage
    - func_sso_test_stage:
        requires:
        - delete_test_data_stage
    - func_profile_test_stage:
        requires:
        - delete_test_data_stage
    - generate_allure_report_from_functional_tests_and_send_link_to_slack:
        requires:
        - func_fas_test_stage
        - func_sso_test_stage
        - func_profile_test_stage
        - func_international_test_stage
  run_browser_tests_in_parallel_in_stage:
    triggers:
    - schedule:
        cron: 23 3 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_parallel_browser_tests_using_circleci
    - stage_parallel_browser_tests_using_circleci:
        requires:
        - setup_env_for_parallel_browser_tests_using_circleci
    - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
        requires:
        - stage_parallel_browser_tests_using_circleci
    - merge_junit_results_from_parallel_browser_tests:
        requires:
        - stage_parallel_browser_tests_using_circleci
    - store_behave_logs_from_browser_tests_as_artifacts:
        requires:
        - stage_parallel_browser_tests_using_circleci
  run_cms_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 53 3 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_cms_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_fas_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 4 4 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_fas_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_isd_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 16 4 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_isd_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_international_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 27 4 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_international_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_invest_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 38 4 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_invest_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_fab_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 49 4 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_fab_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_soo_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 59 4 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_soo_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_domestic_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 11 5 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_domestic_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_erp_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 23 5 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_erp_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_profile_load_tests_on_stage:
    triggers:
    - schedule:
        cron: 23 5 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_load_tests
    - load_profile_tests_stage:
        requires:
        - setup_env_for_load_tests
  run_smoke_and_functional_tests_on_uat:
    triggers:
    - schedule:
        cron: 33 5 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_smoke_tests
    - setup_env_for_functional_tests
    - delete_test_data_uat:
        requires:
        - setup_env_for_functional_tests
    - smoke_tests_uat:
        requires:
        - setup_env_for_smoke_tests
    - func_fas_test_uat:
        requires:
        - delete_test_data_uat
    - func_international_test_uat:
        requires:
        - delete_test_data_uat
    - func_sso_test_uat:
        requires:
        - delete_test_data_uat
    - func_profile_test_uat:
        requires:
        - delete_test_data_uat
    - generate_allure_report_from_functional_tests_and_send_link_to_slack:
        requires:
        - func_fas_test_uat
        - func_sso_test_uat
        - func_profile_test_uat
        - func_international_test_uat
  run_browser_tests_in_parallel_in_uat:
    triggers:
    - schedule:
        cron: 3 6 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_parallel_browser_tests_using_circleci
    - uat_parallel_browser_tests_using_circleci:
        requires:
        - setup_env_for_parallel_browser_tests_using_circleci
    - merge_browser_allure_results_and_generate_report_and_send_link_to_slack:
        requires:
        - uat_parallel_browser_tests_using_circleci
    - merge_junit_results_from_parallel_browser_tests:
        requires:
        - uat_parallel_browser_tests_using_circleci
    - store_behave_logs_from_browser_tests_as_artifacts:
        requires:
        - uat_parallel_browser_tests_using_circleci
  refresh_geckoboard_periodically:
    triggers:
    - schedule:
        cron: 0 8,13 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - refresh_geckoboard
  prod_check_for_dead_links:
    triggers:
    - schedule:
        cron: 43 6 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_prod:
        requires:
        - setup_env_for_periodic_tasks
  uat_check_for_dead_links:
    triggers:
    - schedule:
        cron: 53 6 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_uat:
        requires:
        - setup_env_for_periodic_tasks
  stage_check_for_dead_links:
    triggers:
    - schedule:
        cron: 0 7 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_stage:
        requires:
        - setup_env_for_periodic_tasks
  dev_check_for_dead_links:
    triggers:
    - schedule:
        cron: 10 7 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - check_for_dead_links_on_dev:
        requires:
        - setup_env_for_periodic_tasks
  prod_check_cms_peges_return_200:
    triggers:
    - schedule:
        cron: 17 7 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - check_cms_pages_on_production_return_200:
        requires:
        - setup_env_for_periodic_tasks
  domestic_prod_dev_content_diff:
    triggers:
    - schedule:
        cron: 28 7 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - domestic_compare_prod_and_dev_pages:
        requires:
        - setup_env_for_periodic_tasks
  domestic_prod_uat_content_diff:
    triggers:
    - schedule:
        cron: 40 7 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - domestic_compare_prod_and_uat_pages:
        requires:
        - setup_env_for_periodic_tasks
  domestic_prod_stage_content_diff:
    triggers:
    - schedule:
        cron: 51 7 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - domestic_compare_prod_and_stage_pages:
        requires:
        - setup_env_for_periodic_tasks
  domestic_stage_uat_content_diff:
    triggers:
    - schedule:
        cron: 7 8 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - domestic_compare_stage_and_uat_pages:
        requires:
        - setup_env_for_periodic_tasks
  domestic_stage_dev_content_diff:
    triggers:
    - schedule:
        cron: 21 8 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - domestic_compare_stage_and_dev_pages:
        requires:
        - setup_env_for_periodic_tasks
  international_prod_stage_content_diff:
    triggers:
    - schedule:
        cron: 34 8 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - international_compare_prod_and_stage_pages:
        requires:
        - setup_env_for_periodic_tasks
  international_prod_uat_content_diff:
    triggers:
    - schedule:
        cron: 52 8 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - international_compare_prod_and_uat_pages:
        requires:
        - setup_env_for_periodic_tasks
  international_stage_uat_content_diff:
    triggers:
    - schedule:
        cron: 17 9 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - international_compare_stage_and_uat_pages:
        requires:
        - setup_env_for_periodic_tasks
  international_stage_dev_content_diff:
    triggers:
    - schedule:
        cron: 33 9 * * 3
        filters:
          branches:
            only: master
    jobs:
    - setup_env_for_periodic_tasks
    - international_compare_stage_and_dev_pages:
        requires:
        - setup_env_for_periodic_tasks
  prod_cms_page_status_report:
    triggers:
    - schedule:
        cron: 42 14 * * 1-5
        filters:
          branches:
            only: master
    jobs:
    - generate_cms_page_status_report_for_prod
